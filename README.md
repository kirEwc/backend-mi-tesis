# ğŸ§  Chatbot Backend con NestJS, Ollama y PostgreSQL

Este proyecto es una API desarrollada en **NestJS** que proporciona un chatbot con capacidades de respuesta natural usando un modelo LLM (`qwen:0.5b`) a travÃ©s de **Ollama**. El chatbot se alimenta de una **base de conocimientos contenida en 4 archivos PDF**, procesados en memoria como embeddings, y almacena los historiales de conversaciÃ³n en **PostgreSQL**.

## ğŸ“Œ TecnologÃ­as utilizadas

- **NestJS**: Framework para el backend.
- **Ollama**: Motor local para modelos de lenguaje como `qwen:0.5b`.
- **Angular**: AplicaciÃ³n frontend que consume esta API.
- **PostgreSQL**: Base de datos para almacenar los chats e historial de conversaciÃ³n.
- **Swagger**: Para documentar los endpoints de la API.
- **pdf-parse**: Para extraer texto de documentos PDF.
- **Cosine Similarity**: Para comparar embeddings.
- **En memoria**: Los embeddings no se almacenan en base de datos, solo en RAM.

## ğŸ¯ Flujo general

```text
[ Usuario pregunta ]
        â†“
[ Buscar en PDFs (embedding + similaridad) ]
        â†“
[ Extraer contexto relevante ]
        â†“
[ Generar respuesta con LLM (Ollama/qwen) usando prompt + contexto ]
        â†“
[ Devolver respuesta al usuario ]
        â†“
[ (Futuro) Guardar en historial de chat ]
```

## ğŸ“‚ Estructura del proyecto

```bash
src/
â”‚
â”œâ”€â”€ domain/
â”‚   â”œâ”€â”€ chat/          # Interfaces y entidades para el dominio de chat
â”‚   â””â”€â”€ vector/        # Interfaces para embeddings
â”‚
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ vector/
â”‚   â”‚   â””â”€â”€ in-memory-vector.service.ts # Servicio en memoria para embeddings
â”‚   â”œâ”€â”€ ai/
â”‚   â”‚   â””â”€â”€ ollama-agent.service.ts     # ComunicaciÃ³n con el modelo de Ollama
â”‚
â”œâ”€â”€ application/
â”‚   â””â”€â”€ use-cases/
â”‚       â””â”€â”€ ask-question.use-case.ts    # Caso de uso principal para responder preguntas
â”‚
â”œâ”€â”€ knowledgebase/                      # Carpeta con los PDF base
â”‚
â””â”€â”€ embedding/
    â”œâ”€â”€ embedding.provider.ts
    â””â”€â”€ document-processor.service.ts   # Extrae texto de los PDF y genera embeddings
```

## ğŸš€ Endpoints actuales

| MÃ©todo | Endpoint           | DescripciÃ³n                                 |
|--------|--------------------|---------------------------------------------|
| POST   | `/chat/ask`        | Recibe una pregunta y devuelve una respuesta basada en contexto |
| GET    | `/docs`            | Interfaz Swagger con la documentaciÃ³n       |

## ğŸ§  Prompt base para Qwen

```text
Eres un asistente acadÃ©mico que responde preguntas con base en un contexto dado. Si no sabes la respuesta, simplemente responde que no sabes o que no hay informaciÃ³n suficiente.
```

## ğŸ“¥ InstalaciÃ³n

```bash
git clone <repo>
cd chatbot-backend
yarn install
```

## âš™ï¸ ConfiguraciÃ³n

1. AsegÃºrate de tener **Ollama** instalado y corriendo:
```bash
ollama run qwen:0.5b
```

2. Configura PostgreSQL (solo necesario para guardar chats mÃ¡s adelante).

3. Crea la carpeta `knowledgebase` en la raÃ­z del proyecto y coloca allÃ­ tus 4 PDFs.

## â–¶ï¸ Scripts importantes

```bash
yarn run embed-docs     # Procesa los PDFs y genera embeddings
yarn start:dev          # Ejecuta el proyecto en modo desarrollo
```

## âœ… Checkpoints

- [ ] Procesamiento de PDFs y generaciÃ³n de embeddings en memoria.
- [ ] IntegraciÃ³n con Ollama para respuestas.
- [ ] BÃºsqueda de contexto relevante mediante embeddings.
- [ ] Almacenamiento de conversaciones en PostgreSQL.
- [ ] Endpoint para crear y listar chats.
- [ ] Soporte para mÃºltiples sesiones de conversaciÃ³n.
