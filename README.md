# ğŸ§  Chatbot Backend con NestJS, Ollama y PostgreSQL

Este proyecto es una API desarrollada en **NestJS** que proporciona un chatbot con capacidades de respuesta natural usando un modelo LLM (`qwen:0.5b`) a travÃ©s de **Ollama**. El chatbot se alimenta de una **base de conocimientos contenida en 4 archivos PDF**, procesados en memoria como embeddings, y almacena los historiales de conversaciÃ³n en **PostgreSQL**.

## ğŸ¯ Falta por corregir
-  [ ] Mejorar la busqueda de contexto(similaridad) para que sea mÃ¡s precisa.ya que no se esta haciendo de forma correcta
-  [ ] Mejorar (el prompt para que la ia sepa como devolver la respuesta:)la generaciÃ³n de respuesta para que sea mÃ¡s natural y no tenga que ser exacta.



## ğŸ“Œ TecnologÃ­as utilizadas

- **NestJS**: Framework para el backend.
- **Ollama**: Motor local para modelos de lenguaje como `qwen:0.5b`.
- **Angular**: AplicaciÃ³n frontend que consume esta API.
- **PostgreSQL**: Base de datos para almacenar los chats e historial de conversaciÃ³n.
- **Swagger**: Para documentar los endpoints de la API.
- **pdf-parse**: Para extraer texto de documentos PDF.
- **Cosine Similarity**: Para comparar embeddings.
- **En memoria**: Los embeddings no se almacenan en base de datos, solo en RAM.

## ğŸ¯ Flujo general

```text
[ Usuario pregunta ]
        â†“
[ Buscar en PDFs (embedding + similaridad) ]
        â†“
[ Extraer contexto relevante ]
        â†“
[ Generar respuesta con LLM (Ollama/qwen) usando prompt + contexto ]
        â†“
[ Devolver respuesta al usuario ]
        â†“
[ (Futuro) Guardar en historial de chat ]
```

## ğŸ“‚ Estructura del proyecto

```bash
src/
â”‚
â”œâ”€â”€ domain/
â”‚   â”œâ”€â”€ chat/          # Interfaces y entidades para el dominio de chat
â”‚   â””â”€â”€ vector/        # Interfaces para embeddings
â”‚
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ vector/
â”‚   â”‚   â””â”€â”€ in-memory-vector.service.ts # Servicio en memoria para embeddings
â”‚   â”œâ”€â”€ ai/
â”‚   â”‚   â””â”€â”€ ollama-agent.service.ts     # ComunicaciÃ³n con el modelo de Ollama
â”‚
â”œâ”€â”€ application/
â”‚   â””â”€â”€ use-cases/
â”‚       â””â”€â”€ ask-question.use-case.ts    # Caso de uso principal para responder preguntas
â”‚
â”œâ”€â”€ knowledgebase/                      # Carpeta con los PDF base
â”‚
â””â”€â”€ embedding/
    â”œâ”€â”€ embedding.provider.ts
    â””â”€â”€ document-processor.service.ts   # Extrae texto de los PDF y genera embeddings
```

## ğŸš€ Endpoints actuales

| MÃ©todo | Endpoint           | DescripciÃ³n                                 |
|--------|--------------------|---------------------------------------------|
| POST   | `/chat/ask`        | Recibe una pregunta y devuelve una respuesta basada en contexto |
| GET    | `/docs`            | Interfaz Swagger con la documentaciÃ³n       |

## ğŸ§  Prompt base para Qwen

```text
Eres un asistente acadÃ©mico que responde preguntas con base en un contexto dado. Si no sabes la respuesta, simplemente responde que no sabes o que no hay informaciÃ³n suficiente.
```

## ğŸ“¥ InstalaciÃ³n

```bash
git clone <repo>
cd chatbot-backend
yarn install
```

## âš™ï¸ ConfiguraciÃ³n

1. AsegÃºrate de tener **Ollama** instalado y corriendo:
```bash
ollama run qwen:0.5b
```

2. Configura PostgreSQL (solo necesario para guardar chats mÃ¡s adelante).

3. Crea la carpeta `knowledgebase` en la raÃ­z del proyecto y coloca allÃ­ tus 4 PDFs.

## â–¶ï¸ Scripts importantes

```bash
yarn run embed-docs     # Procesa los PDFs y genera embeddings
yarn start:dev          # Ejecuta el proyecto en modo desarrollo
```

## âœ… Checkpoints

### âœ… CHECKPOINT 1 â€“ "Pregunta â†” Respuesta Funcional"
#### - [ ] SecciÃ³n 1.1 â€“ Carga de PDFs
Crear carpeta knowledgebase/

Leer los 4 PDFs desde esa carpeta

Convertir PDFs a texto usando pdf-parse, pdfjs-dist o el loader de LangChain

Dejar los textos como Document[] de LangChain

#### - [ ] SecciÃ³n 1.2 â€“ IndexaciÃ³n y bÃºsqueda con LangChain
Indexar los documentos (puede ser con MemoryVectorStore o HNSWLib)

Usar Embedding de OpenAI vÃ­a LangChain con OpenRouter

Implementar bÃºsqueda semÃ¡ntica de contexto por pregunta

#### - [ ] SecciÃ³n 1.3 â€“ PreparaciÃ³n del Prompt y llamada a OpenRouter
Definir el prompt (puede estar en un archivo .txt o como string constante)

Armar el messages con context + prompt

Llamar a la API de OpenRouter (modelo DeepSeek) y recibir respuesta

#### - [ ] SecciÃ³n 1.4 â€“ Endpoint funcional + Swagger
Crear POST /chat/ask en NestJS

Validar que pregunta estÃ© en el body

Enviar pregunta, buscar contexto, generar respuesta y devolverla

Documentar todo con Swagger (input, output, ejemplos)

### âœ… CHECKPOINT 2 â€“ "Historial de Chats"
#### - [ ] SecciÃ³n 2.1 â€“ Modelo de datos (PostgreSQL + TypeORM)
Entidad Chat (id, fecha creaciÃ³n, tÃ­tulo)

Entidad Message (id, chatId, pregunta, respuesta, timestamps)

#### - [ ] SecciÃ³n 2.2 â€“ Endpoints CRUD
POST /chat â†’ crear nuevo chat

GET /chat/:id â†’ obtener historial

GET /chat â†’ listar chats

DELETE /chat/:id â†’ eliminar un chat

#### - [ ] SecciÃ³n 2.3 â€“ VinculaciÃ³n con flujo del chat
Modificar POST /chat/:chatId/ask para:

guardar pregunta

buscar contexto

generar respuesta

guardar respuesta

devolver resultado

#### - [ ] SecciÃ³n 2.4 â€“ DocumentaciÃ³n Swagger completa de estos endpoints

### âœ… CHECKPOINT 3 â€“ "IntegraciÃ³n Angular + DocumentaciÃ³n Final"
#### - [ ] SecciÃ³n 3.1 â€“ CORS + ConfiguraciÃ³n final de NestJS
Habilitar CORS

Asegurar seguridad mÃ­nima (throttling, headers, etc)

#### - [ ] SecciÃ³n 3.2 â€“ ConexiÃ³n desde Angular
Frontend llama a: crear chat, ver historial, hacer pregunta

UX bÃ¡sica para mostrar respuestas del chatbot

#### - [ ] SecciÃ³n 3.3 â€“ Swagger completo y limpio
Descripciones, tags, ejemplos, esquemas DTOs bien definidos